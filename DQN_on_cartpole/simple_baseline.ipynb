{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eefbd8cc",
   "metadata": {},
   "source": [
    "IMPORT DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b243fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1d43c1",
   "metadata": {},
   "source": [
    "LOAD ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "936cea77",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name= \"CartPole-v0\"\n",
    "env= gym.make(env_name, render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "d700c6f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Score: 15.0\n",
      "Episode 2 Score: 26.0\n",
      "Episode 3 Score: 23.0\n",
      "Episode 4 Score: 12.0\n",
      "Episode 5 Score: 43.0\n"
     ]
    }
   ],
   "source": [
    "episodes = 5 #number of times the whole environment is run\n",
    "for episode in range(1, episodes + 1):\n",
    "    state= env.reset() # reset the environment to the initial state.... it will return the initial state value which is the first observation\n",
    "    done = False # done is a boolean that indicates whether the episode has ended\n",
    "    score = 0 # initialize the score for the episode\n",
    "    \n",
    "    while not done:\n",
    "        env.render() # render   the environment to visualize the actions taken\n",
    "        action= env.action_space.sample() # take a random action from the action space\n",
    "        state, reward, terminated , done, info  = env.step(action)  # apply the action to the environment\n",
    "        # state is the new state after taking the action\n",
    "        # reward is the reward received for taking the action\n",
    "        # terminated is a boolean that indicates whether the episode has ended due to a terminal state\n",
    "        # done is a boolean that indicates whether the episode has ended\n",
    "        # info is a dictionary that contains additional information about the environment\n",
    "        score += reward \n",
    "\n",
    "    print(f\"Episode {episode} Score: {score}\")\n",
    "env.close() # close the render frame "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c09a0ff5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "79b8e921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.0918093,  4.37141  , -1.7520808, -8.110683 ], dtype=float32),\n",
       " 0.0,\n",
       " True,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c5e18",
   "metadata": {},
   "source": [
    "UNDERSTANDING ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "d92b7fa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "9f10506a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "68f532e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "bd1e8761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.45313802, -0.63500464, -0.2454992 , -0.98092854], dtype=float32)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0572388",
   "metadata": {},
   "source": [
    "TRAINING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "865bf80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_path = os.path.join('Training', 'Logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "4d38fb90",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c41850",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, render_mode=\"human\") #render_mode=\"human\" is used to visualize the environment\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVec because Stable Baselines3 requires a vectorized environment for training\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=log_path)  # Create the PPO model with MLP policy which is a Multi-Layer Perceptron\n",
    "# verbose=1 will print the training progress\n",
    "# tensorboard_log=log_path will log the training progress to the specified path for visualization in TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "26ee640c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to Training\\Logs\\PPO_1\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 45   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 45   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 90          |\n",
      "|    total_timesteps      | 4096        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008916764 |\n",
      "|    clip_fraction        | 0.0875      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.686      |\n",
      "|    explained_variance   | -0.00335    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.5         |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0125     |\n",
      "|    value_loss           | 50.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 134         |\n",
      "|    total_timesteps      | 6144        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008098634 |\n",
      "|    clip_fraction        | 0.0489      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.672      |\n",
      "|    explained_variance   | 0.0749      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 15.2        |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.0137     |\n",
      "|    value_loss           | 39.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 178         |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008584155 |\n",
      "|    clip_fraction        | 0.0916      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.638      |\n",
      "|    explained_variance   | 0.232       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 21.7        |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.0199     |\n",
      "|    value_loss           | 53.8        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 45          |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 222         |\n",
      "|    total_timesteps      | 10240       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008641107 |\n",
      "|    clip_fraction        | 0.0955      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.609      |\n",
      "|    explained_variance   | 0.317       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 33.4        |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.0208     |\n",
      "|    value_loss           | 62.7        |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 266         |\n",
      "|    total_timesteps      | 12288       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.011015162 |\n",
      "|    clip_fraction        | 0.0862      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.602      |\n",
      "|    explained_variance   | 0.414       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 22.9        |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 56          |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 310         |\n",
      "|    total_timesteps      | 14336       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.006630755 |\n",
      "|    clip_fraction        | 0.063       |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.587      |\n",
      "|    explained_variance   | 0.471       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 16.8        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.0126     |\n",
      "|    value_loss           | 57.5        |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 8            |\n",
      "|    time_elapsed         | 353          |\n",
      "|    total_timesteps      | 16384        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0053617335 |\n",
      "|    clip_fraction        | 0.0448       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.558       |\n",
      "|    explained_variance   | 0.233        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 17.6         |\n",
      "|    n_updates            | 70           |\n",
      "|    policy_gradient_loss | -0.00665     |\n",
      "|    value_loss           | 61.8         |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| time/                   |             |\n",
      "|    fps                  | 46          |\n",
      "|    iterations           | 9           |\n",
      "|    time_elapsed         | 397         |\n",
      "|    total_timesteps      | 18432       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.005819623 |\n",
      "|    clip_fraction        | 0.0331      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.555      |\n",
      "|    explained_variance   | 0.149       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 10.8        |\n",
      "|    n_updates            | 80          |\n",
      "|    policy_gradient_loss | -0.00287    |\n",
      "|    value_loss           | 28          |\n",
      "-----------------------------------------\n",
      "------------------------------------------\n",
      "| time/                   |              |\n",
      "|    fps                  | 46           |\n",
      "|    iterations           | 10           |\n",
      "|    time_elapsed         | 440          |\n",
      "|    total_timesteps      | 20480        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0063892133 |\n",
      "|    clip_fraction        | 0.0126       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.566       |\n",
      "|    explained_variance   | 0.724        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 0.979        |\n",
      "|    n_updates            | 90           |\n",
      "|    policy_gradient_loss | -0.000583    |\n",
      "|    value_loss           | 14.2         |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x2086ee68650>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.learn(total_timesteps=20000)  # Train the model for 20,000 timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a03484",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()  # close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2866ecd",
   "metadata": {},
   "source": [
    "SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "6558e12c",
   "metadata": {},
   "outputs": [],
   "source": [
    "PPO_path = os.path.join('Training', 'SavedModels', 'PPO_CartPole_Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "1dc31d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(PPO_path)  # Save the model to the specified path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "a6f97fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model  # delete the model to free up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "477ac984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model = PPO.load(PPO_path, env=env)  # Load the model from the specified path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8c915",
   "metadata": {},
   "source": [
    "EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "87ec1af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200.0, 0.0)"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_policy( model, env, n_eval_episodes= 10, render=True)  # Evaluate the model on 10 episodes and render the environment)\n",
    "# output will be the mean reward and standard deviation of the reward over the 10 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "c54424f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()  # close the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537fbceb",
   "metadata": {},
   "source": [
    "TESTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "123d725d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1 Score: [200.]\n",
      "Episode 2 Score: [200.]\n",
      "Episode 3 Score: [127.]\n",
      "Episode 4 Score: [200.]\n",
      "Episode 5 Score: [200.]\n"
     ]
    }
   ],
   "source": [
    "episodes = 5 \n",
    "env= gym.make(env_name, render_mode=\"human\")  # create the environment with human rendering\n",
    "env = DummyVecEnv([lambda: env])  \n",
    "\n",
    "for episode in range(1, episodes + 1): \n",
    "    state = env.reset()  # reset the environment to the initial state\n",
    "    done = False  # done is a boolean that indicates whether the episode has ended\n",
    "    score = 0  # initialize the score for the episode\n",
    "    \n",
    "    while not done:\n",
    "        env.render()\n",
    "        action, _states = model.predict(state)  # use the model to predict the action to take based on the current state\n",
    "        state, reward, done, info = env.step(action)  # apply the action to the environment\n",
    "        score += reward  # accumulate the score\n",
    "\n",
    "    print(f\"Episode {episode} Score: {score}\")  # print the score for the episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "3d63fb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "62c87bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(env_name, render_mode=\"human\")  # create the environment with human rendering\n",
    "env = DummyVecEnv([lambda: env])  # Wrap the environment in a DummyVec\n",
    "obs=env.reset()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "48375c0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0], dtype=int64), None)"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(obs)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d98892d",
   "metadata": {},
   "source": [
    "VIEWING LOGS IN TENSORBOARD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "948b3079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Training\\\\Logs\\\\PPO_1'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_log_path = os.path.join(log_path, 'PPO_1')\n",
    "training_log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "de17f63f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir={training_log_path}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69997d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da7f6a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2fbe6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
